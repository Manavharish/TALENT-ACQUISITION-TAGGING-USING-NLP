{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Description Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2348449629.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install gensim\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install gensim\n",
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpTokenizer, word_tokenize\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m  \u001b[38;5;66;03m# pip install python-docx\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# Import Module\n",
    "import os\n",
    "import hashlib\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "import docx  # pip install python-docx\n",
    "import fitz  # pip install PyMuPDF\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path\n",
    "path = \"C:/Users/PSNR/Documents/Resume_project/Job_Descript\"\n",
    "\n",
    "os.chdir(\"C:/Users/PSNR/Documents/Resume_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert tokens to bigram words\n",
    "def get_bigrams(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return zip(tokens, tokens[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the bigram tokens from text\n",
    "def filter_bigram_tokens(bigram_list, bigram_tokens):\n",
    "    token=[]\n",
    "    for i in range(len(bigram_tokens)):\n",
    "        if bigram_tokens[i] in bigram_list:\n",
    "            token.append(bigram_tokens[i])\n",
    "            i+=2\n",
    "        else:\n",
    "            word = bigram_tokens[i][:bigram_tokens[i].find(\" \")]\n",
    "            token.append(word)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Model using job description Text\n",
    "def built_model(bigram_list):\n",
    "    os.chdir(path)\n",
    "    filename = []\n",
    "    file_text = []\n",
    "    frequent_word_list = []\n",
    "    model_name=[]\n",
    "    model_keywords_list=[]\n",
    "\n",
    "    print(\"\\n\\n Processing the Job Description\\n++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "    # iterate through all file\n",
    "    for file in os.listdir():\n",
    "        # Check whether file is in text format or not\n",
    "        file_path = f\"{path}\\{file}\"\n",
    "        text = []\n",
    "        if file.endswith(\".docx\"):\n",
    "\n",
    "            doc = docx.Document(file_path)\n",
    "            # Reading the document content\n",
    "            for para in doc.paragraphs:\n",
    "                text.append(para.text)\n",
    "\n",
    "        if file.endswith(\".pdf\"):\n",
    "            with fitz.open(file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text.append(page.get_text())\n",
    "        print(f\"\\n\\nfile: {file}\")\n",
    "\n",
    "\n",
    "        x=[]\n",
    "        for line in text:\n",
    "\n",
    "\n",
    "            tokens = word_tokenize(line)\n",
    "            tok = [w.lower() for w in tokens]\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            strpp = [w.translate(table) for w in tok]\n",
    "            words = [word for word in strpp if word.isalpha()]\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            words = [w for w in words if not w in stop_words]\n",
    "            token_text = \" \".join(words)\n",
    "            bigram_tokens = [' '.join(b) for b in get_bigrams(token_text.lower())]\n",
    "            f_tokens = filter_bigram_tokens(bigram_list, bigram_tokens)\n",
    "            lemmatizer = WordNetLemmatizer()\n",
    "            lem_sent = [lemmatizer.lemmatize(words_sent) for words_sent in f_tokens]\n",
    "            x.append(lem_sent)\n",
    "\n",
    "\n",
    "        ### Building model\n",
    "\n",
    "        # finding unique values\n",
    "        texts = x\n",
    "        ntexts =[]\n",
    "        for line in texts:\n",
    "            uniq_words = set(line)\n",
    "            unique_list = []\n",
    "            for w in uniq_words:\n",
    "                if w not in unique_list:\n",
    "                    unique_list.append(w)\n",
    "            ntexts.append(unique_list)\n",
    "            # Creating bigrams\n",
    "        common_term = [\"of\", \"with\", \"without\", \"and\", \"or\", \"the\", \"a\",\"within\"]\n",
    "\n",
    "        x = ntexts\n",
    "            # Create the relevant phrases from the list of sentences:\n",
    "        phrases = Phrases(x, connector_words=common_term)\n",
    "            # The Phraser object is used from now on to transform sentences\n",
    "        bigram = Phraser(phrases)\n",
    "            # Applying the Phraser to transform our sentences\n",
    "        sentences = list(bigram[x])\n",
    "            # Building word2vector model\n",
    "        model = gensim.models.Word2Vec(sentences, min_count=2, workers=4, window=4, vector_size=500)\n",
    "            # Saving the model\n",
    "        os.chdir(\"C:/Users/PSNR/Documents/Resume_project/Models\")\n",
    "        model.save(f\"{file}final.model\")\n",
    "        wrds = list(model.wv.index_to_key)\n",
    "        print(f\"model Tagwords: {wrds}\")\n",
    "\n",
    "        # 15 frequent words in the text     if it is not needed then, comment or delete from 115 to 128\n",
    "        # Tokenizing the text\n",
    "        tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        tokens = tokenizer.tokenize(\" \".join(text))\n",
    "        tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "        token_text = \" \".join(tokens)\n",
    "        bigram_tokens = [' '.join(b) for b in get_bigrams(token_text.lower())]\n",
    "        f_tokens = filter_bigram_tokens(bigram_list, bigram_tokens)\n",
    "        print(\"\\nFrequent words:\")\n",
    "        freq = Counter(f_tokens)\n",
    "        max_list = [i for i in sorted(freq.values(),reverse=True)]\n",
    "        for key, value in freq.items():\n",
    "            if value in max_list[:10]:\n",
    "                print(key, \":\", value)\n",
    "\n",
    "        filename.append(file)\n",
    "        file_text.append(text)\n",
    "        frequent_word_list.append(freq)\n",
    "        model_name.append(f\"{file}final.model\")\n",
    "        model_keywords_list.append(wrds)\n",
    "\n",
    "    # Change the directory\n",
    "    os.chdir(\"C:/Users/PSNR/Documents/Resume_project\")\n",
    "    # dictionary of lists\n",
    "    job_dict = {'File name': filename, 'Text': file_text,'frequent_word_list':frequent_word_list,'Model_name':model_name,'Model_keywords_list':model_keywords_list}\n",
    "    df = pd.DataFrame(job_dict)\n",
    "    # saving the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/PSNR/Documents/Resume_project\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m bigram_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# retreiving the bigram words\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir(\"C:/Users/PSNR/Documents/Resume_project\")\n",
    "bigram_list = []\n",
    "# retreiving the bigram words\n",
    "f = open('bigram_words.txt', 'r')\n",
    "word_list = f.readlines()\n",
    "for sub in word_list:\n",
    "    bigram_list.append(re.sub('\\n', '', sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "built_model(bigram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
