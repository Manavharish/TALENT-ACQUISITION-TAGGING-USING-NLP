{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESUME PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx in /Users/anudeepchowdary/opt/anaconda3/lib/python3.9/site-packages (0.2.4)\n",
      "Requirement already satisfied: lxml in /Users/anudeepchowdary/opt/anaconda3/lib/python3.9/site-packages (from docx) (4.6.3)\n",
      "Requirement already satisfied: Pillow>=2.0 in /Users/anudeepchowdary/opt/anaconda3/lib/python3.9/site-packages (from docx) (8.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/anudeepchowdary/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import Module\n",
    "import os\n",
    "import hashlib\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import docx  # pip install python-docx\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path\n",
    "path = \"/Users/anudeepchowdary/Desktop/pre_resume/Resumes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the duplicate files\n",
    "def remove_duplicate():\n",
    "    unique_files = []\n",
    "    os.chdir(\"/Users/anudeepchowdary/Desktop/pre_resume/Resumes\")\n",
    "    for filename in os.listdir():\n",
    "        if os.path.isfile(filename):\n",
    "            filehash = hashlib.md5(open(\n",
    "            filename,'rb').read()).hexdigest()\n",
    "            if filehash not in unique_files:\n",
    "                unique_files.append(filehash)\n",
    "            else:\n",
    "                os.remove(filename)\n",
    "                print(f\"{filename} has been deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert tokens to bigram words\n",
    "def get_bigrams(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    return zip(tokens, tokens[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the bigram tokens from text\n",
    "def filter_bigram_tokens(bigram_list, bigram_tokens):\n",
    "    token=[]\n",
    "    for i in range(len(bigram_tokens)):\n",
    "        if bigram_tokens[i] in bigram_list:\n",
    "            token.append(bigram_tokens[i])\n",
    "            i+=2\n",
    "        else:\n",
    "            word = bigram_tokens[i][:bigram_tokens[i].find(\" \")]\n",
    "            token.append(word)\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_processing(bigram_list):\n",
    "    # Change the directory\n",
    "    os.chdir(path)\n",
    "    filename = []\n",
    "    resume_text = []\n",
    "    tokens_list = []\n",
    "    pre_token = []\n",
    "\n",
    "    print(\"\\n\\n Resume Processing\\n++++++++++++++++++++++++++++++++++++++++++\\n\")\n",
    "    # iterate through all file\n",
    "    for file in os.listdir():\n",
    "\n",
    "        # Check whether file is in text format or not\n",
    "        if file.endswith(\".docx\"):\n",
    "            file_path = f\"{path}\\{file}\"\n",
    "            text = []\n",
    "            try:\n",
    "                doc = docx.Document(file_path)\n",
    "\n",
    "                #Reading the document content\n",
    "                for para in doc.paragraphs:\n",
    "                    text.append(para.text)\n",
    "                print(f\"file: {file}\")\n",
    "\n",
    "                # Tokenizing the text\n",
    "                tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                tokens = tokenizer.tokenize(\" \".join(text))\n",
    "                tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "                # Removing the duplicate file with similar tokens and filtering bigram words\n",
    "                token_text = \" \".join(tokens)\n",
    "                bigram_tokens = [' '.join(b) for b in get_bigrams(token_text.lower())]\n",
    "                f_tokens = filter_bigram_tokens(bigram_list, bigram_tokens)\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                lem_sent = [lemmatizer.lemmatize(words_sent) for words_sent in f_tokens]\n",
    "                if pre_token[:10] != lem_sent[:10] and len(lem_sent)!=0:\n",
    "                    # appending the text\n",
    "                    print(lem_sent)\n",
    "                    filename.append(file)\n",
    "                    resume_text.append(text)\n",
    "                    tokens_list.append(lem_sent)\n",
    "                    pre_token=lem_sent\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Change the directory\n",
    "    os.chdir(\"/Users/anudeepchowdary/Desktop/pre_resume\")\n",
    "    # dictionary of lists\n",
    "    resume_dict = {'File name': filename, 'Text': resume_text, 'Token_list': tokens_list}\n",
    "    df = pd.DataFrame(resume_dict)\n",
    "    # saving the dataframe\n",
    "    df.to_csv('Resume_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_duplicate()\n",
    "os.chdir(\"/Users/anudeepchowdary/Desktop/pre_resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_list= []\n",
    "# retreiving the bigram words\n",
    "f = open('bigram_words.txt', 'r')\n",
    "word_list = f.readlines()\n",
    "for sub in word_list:\n",
    "    bigram_list.append(re.sub('\\n', '', sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Resume Processing\n",
      "++++++++++++++++++++++++++++++++++++++++++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_processing(bigram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
